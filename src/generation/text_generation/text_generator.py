"""
ê´‘ê³  ë¬¸êµ¬ ìƒì„± ëª¨ë“ˆ
ì‘ì„±ì: ë°°í˜„ì„
ë²„ì „: 1.1 (ì‹ ìŠ¹ëª©, ë¡œê¹… ì¶”ê°€)
"""

import os
import re
from typing import Optional, Tuple
from dotenv import load_dotenv
from openai import OpenAI

from src.generation.text_generation.prompt_manager import PromptTemplateManager
from src.utils.logging import get_logger

logger = get_logger(__name__)
load_dotenv()


class TextGenerator:
    """ê´‘ê³  ë¬¸êµ¬ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self, use_industry_config: bool = True):
        """
        ì´ˆê¸°í™”: OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

        Args:
            use_industry_config (bool): Trueì´ë©´ PromptTemplateManagerë¥¼ ì‚¬ìš©í•˜ì—¬
                                        ì—…ì¢…ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4o-mini"
        self.use_industry_config = use_industry_config
        self.prompt_manager = PromptTemplateManager()

    def generate_ad_copy(
        self,
        user_input,
        tone="warm",
        max_length=100,
        chat_history=None,
        generation_history=None,
        industry=None,
    ):
        """
        ê´‘ê³  ë¬¸êµ¬ ìƒì„±

        Args:
            user_input (str): ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸
                ì˜ˆ: "ì¹´í˜ ì‹ ë©”ë‰´ í™ë³´, ë”°ëœ»í•œ ëŠë‚Œ, ê²¨ìš¸"
            tone (str): í†¤ ì•¤ ë§¤ë„ˆ ("warm", "professional", "friendly", "energetic")
            max_length (int): ìµœëŒ€ ê¸€ì ìˆ˜ (ê¸°ë³¸ 100ì)
            chat_history (list): ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)
            generation_history (list): ì´ì „ ìƒì„± ì´ë ¥ (ì„ íƒ)
            industry (str): ì—…ì¢… ì •ë³´ (cafe, restaurant, gym ë“±, ì„ íƒ)
        Returns:
            str: ìƒì„±ëœ ê´‘ê³  ë¬¸êµ¬
                ì˜ˆ: "ë”°ëœ»í•œ ê²¨ìš¸, ìƒˆë¡œìš´ ë§›"
        """

        logger.info("ğŸ“ ê´‘ê³  ë¬¸êµ¬ ìƒì„± ì¤‘...")
        logger.info(f"   ì…ë ¥: {user_input}")
        logger.info(f"   í†¤: {tone}, ìµœëŒ€ {max_length}ì, ì—…ì¢…: {industry or 'general'}")
        logger.info(f"   ì—…ì¢… ì„¤ì • ì‚¬ìš©: {self.use_industry_config}")
        try:
            length_spec = self._extract_length_expectation(
                self._merge_length_source_text(user_input)
            )
            target_min = length_spec.min_len
            target_max = length_spec.max_len
            target_source = length_spec.source
            wants_long = length_spec.is_long

            safe_max_length = 100
            if max_length is not None:
                try:
                    safe_max_length = int(max_length)
                except (TypeError, ValueError):
                    safe_max_length = 100
            elif target_max is not None:
                safe_max_length = int(target_max)

            if safe_max_length < 10:
                safe_max_length = 10

            if target_source in ("range", "approx", "exact") and target_max is not None:
                if max_length is not None and int(max_length) < target_max:
                    logger.warning(
                        "length override: spec length=%s exceeds intent max_length=%s",
                        target_max,
                        max_length,
                    )
                safe_max_length = max(safe_max_length, target_max)

            if wants_long and target_source != "exact":
                if max_length is not None and int(max_length) < 600:
                    logger.warning(
                        "length override: user wants long-form but intent max_length=%s",
                        max_length,
                    )
                target_max = min(800, max(600, target_max or 0))
                if target_min is None:
                    target_min = int(target_max * 0.7)
                safe_max_length = max(safe_max_length, target_max)

            if not wants_long and target_max is not None and target_max > safe_max_length:
                logger.warning(
                    "length conflict: target_max=%s exceeds max_length=%s (source=%s)",
                    target_max,
                    safe_max_length,
                    target_source,
                )
                target_max = safe_max_length

            if target_min is not None and target_min > safe_max_length:
                logger.warning(
                    "length conflict: target_min=%s exceeds max_length=%s (source=%s)",
                    target_min,
                    safe_max_length,
                    target_source,
                )
                target_min = max(10, int(safe_max_length * 0.9))

            min_length = target_min if target_min is not None else max(10, int(safe_max_length * 0.7))

            require_hashtags, hashtag_count = self._detect_hashtag_requirement(
                user_input=user_input,
                chat_history=chat_history,
            )

            # ì—…ì¢…ë³„ ì„¤ì • ì‚¬ìš© ì‹œ PromptTemplateManager í™œìš©
            history_context = self._build_history_context(
                chat_history=chat_history,
                generation_history=generation_history,
            )

            for attempt in range(2):
                if self.use_industry_config and self.prompt_manager:
                    prompts = self.prompt_manager.get_ad_copy_prompt(
                        user_input=user_input,
                        tone=tone,
                        max_length=safe_max_length,
                        industry=industry
                    )
                    system_prompt = self._append_length_emphasis(
                        prompts["system_prompt"], safe_max_length, min_length
                    )
                    if require_hashtags:
                        system_prompt = self._append_hashtag_instruction(
                            system_prompt, hashtag_count
                        )
                    user_prompt = prompts["user_prompt"]
                    user_prompt = self._append_user_requirements(
                        user_prompt,
                        max_length=safe_max_length,
                        min_length=min_length,
                        hashtag_count=hashtag_count if require_hashtags else None,
                    )
                    if history_context:
                        user_prompt = f"{user_prompt}\n\n{history_context}"
                    # ìë™ ê°ì§€ëœ ì—…ì¢… ì •ë³´ ì¶œë ¥
                    detected_industry = prompts.get("industry", industry)
                    logger.info(f"   ê°ì§€ëœ ì—…ì¢…: {detected_industry}")
                else:
                    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ì—…ì¢… ì •ë³´ í¬í•¨)
                    system_prompt = self._get_system_prompt(
                        tone,
                        safe_max_length,
                        chat_history=chat_history,
                        generation_history=generation_history,
                        industry=industry,
                    )
                    if require_hashtags:
                        system_prompt = self._append_hashtag_instruction(
                            system_prompt, hashtag_count
                        )

                    # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    user_prompt = self._build_user_prompt(
                        user_input,
                        safe_max_length,
                        min_length=min_length,
                        history_context=history_context,
                        hashtag_count=hashtag_count if require_hashtags else None,
                    )

                # 3. GPT API í˜¸ì¶œ
                if wants_long:
                    max_tokens = min(1500, max(900, int(safe_max_length * 2)))
                else:
                    max_tokens = min(900, max(120, int(safe_max_length * 1.5)))
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=max_tokens
                )

                # 4. ì‘ë‹µ ì¶”ì¶œ
                ad_copy = response.choices[0].message.content.strip()

                # 5. í›„ì²˜ë¦¬
                ad_copy = self._postprocess(ad_copy, safe_max_length)

                too_short = len(ad_copy) < min_length
                missing_hashtags = require_hashtags and ("#" not in ad_copy)
                if too_short or missing_hashtags:
                    logger.debug(
                        "postprocess check failed (attempt=%s): too_short=%s, missing_hashtags=%s",
                        attempt + 1,
                        too_short,
                        missing_hashtags,
                    )
                    if attempt == 0:
                        continue

                logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {ad_copy}")
                return ad_copy

            logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {ad_copy}")
            return ad_copy

        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._get_fallback_copy()

    def _get_system_prompt(
        self,
        tone,
        max_length,
        chat_history=None,
        generation_history=None,
        industry=None,
    ):
        """í†¤, ëŒ€í™” íˆìŠ¤í† ë¦¬, ì—…ì¢…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""

        base_prompt = f"""ë‹¹ì‹ ì€ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ê´‘ê³  ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤.
ì§§ê³  ì„íŒ©íŠ¸ ìˆëŠ” ê´‘ê³  ë¬¸êµ¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ê·œì¹™:
- ë°˜ë“œì‹œ {max_length}ì ì´ë‚´ (ê³µë°± í¬í•¨, ì´ˆê³¼ ê¸ˆì§€)
- ë„ˆë¬´ ì§§ê²Œ ì“°ì§€ ë§ê³  ìµœì†Œí•œì˜ ê¸¸ì´ë¥¼ í™•ë³´í•  ê²ƒ
- ë²ˆí˜¸, íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ë¬¸êµ¬ë§Œ ì‘ì„±
- ì‚¬ìš©ì ë³„ë‹¤ë¥¸ ìš”ì²­ ì—†ì„ì‹œ ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì‚¬ìš©ì ìš”ì²­ì‹œ ìš”ì²­í•œ ì–¸ì–´ë¡œ ì‘ì„±
- ê´‘ê³  ë¬¸êµ¬ 1ê°œë§Œ ìƒì„±"""

        tone_styles = {
            "warm": "ë”°ëœ»í•˜ê³  ê°ì„±ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. í¸ì•ˆí•˜ê³  ì•„ëŠ‘í•œ ëŠë‚Œì„ ì£¼ì„¸ìš”.",
            "professional": "ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²©ì‹ ìˆê³  ì„¸ë ¨ëœ ëŠë‚Œì„ ì£¼ì„¸ìš”.",
            "friendly": "ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ëŒ€í™”í•˜ë“¯ ìì—°ìŠ¤ëŸ¬ìš´ ëŠë‚Œì„ ì£¼ì„¸ìš”.",
            "energetic": "í™œê¸°ì°¨ê³  ì—­ë™ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì—´ì •ì ì´ê³  ê¸ì •ì ì¸ ëŠë‚Œì„ ì£¼ì„¸ìš”."
        }

        tone_guide = tone_styles.get(tone, tone_styles["warm"])

        # ì—…ì¢…ë³„ ê°€ì´ë“œë¼ì¸ ì¶”ê°€ (30ê°œ)
        industry_guides = {
            # 1-10: ê¸°ì¡´ ì—…ì¢…
            "cafe": "ì¹´í˜ëŠ” 'íœ´ì‹', 'ì—¬ìœ ', 'ê°ì„±', 'ë”°ëœ»í•¨', 'ì¼ìƒ ì† íŠ¹ë³„í•œ ìˆœê°„'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì»¤í”¼ í–¥, ì•„ëŠ‘í•œ ê³µê°„, íë§ì˜ ëŠë‚Œì„ ì‚´ë¦½ë‹ˆë‹¤.",
            "restaurant": "ìŒì‹ì ì€ 'ë§›', 'ì˜¤ê°', 'ê²½í—˜', 'ë§Œì¡±ê°', 'íŠ¹ë³„í•œ í•œ ë¼'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ì‹ ì„ í•¨, ì •ì„±, í’ë¯¸ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "bakery": "ë² ì´ì»¤ë¦¬ëŠ” 'ê°“ êµ¬ìš´', 'ì‹ ì„ í•¨', 'ë‹¬ì½¤í•¨', 'í–‰ë³µ', 'ì¼ìƒì˜ ì‘ì€ ì‚¬ì¹˜'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ë¹µ êµ½ëŠ” í–¥ê¸°ì™€ ë”°ëœ»í•¨ì„ ì „í•©ë‹ˆë‹¤.",
            "gym": "í—¬ìŠ¤ì¥ì€ 'ë³€í™”', 'ë„ì „', 'ì„±ì·¨', 'ê±´ê°•', 'ìƒˆë¡œìš´ ë‚˜'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ë™ê¸°ë¶€ì—¬ì™€ ê¸ì •ì  ë³€í™”ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "beauty": "ë·°í‹°/ë¯¸ìš©ì€ 'ì•„ë¦„ë‹¤ì›€', 'ìì‹ ê°', 'ì¼€ì–´', 'ìŠ¤íƒ€ì¼', 'ë‚˜ë¥¼ ìœ„í•œ íˆ¬ì'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ë³€í™”ì™€ ìê¸°ê´€ë¦¬ì˜ ê°€ì¹˜ë¥¼ ì „í•©ë‹ˆë‹¤.",
            "fashion": "íŒ¨ì…˜/ì˜ë¥˜ëŠ” 'ìŠ¤íƒ€ì¼', 'íŠ¸ë Œë“œ', 'ê°œì„±', 'ë©‹', 'ë‚˜ë§Œì˜ í‘œí˜„'ì„ ê°•ì¡°í•˜ì„¸ìš”. ê°ê°ê³¼ ì°¨ë³„í™”ë¥¼ ì‚´ë¦½ë‹ˆë‹¤.",
            "hair_salon": "ë¯¸ìš©ì‹¤ì€ 'ìŠ¤íƒ€ì¼ ë³€ì‹ ', 'í—¤ì–´ ì¼€ì–´', 'ì „ë¬¸ì„±', 'íŠ¸ë Œë””í•¨'ì„ ê°•ì¡°í•˜ì„¸ìš”. ìƒˆë¡œìš´ ì´ë¯¸ì§€ì™€ ë§Œì¡±ê°ì„ ì „í•©ë‹ˆë‹¤.",
            "nail_salon": "ë„¤ì¼ìƒµì€ 'ì„¬ì„¸í•¨', 'ì•„ë¦„ë‹¤ìš´ ë””í…Œì¼', 'ê°ê°', 'ìê¸° í‘œí˜„'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì†ëì˜ ì˜ˆìˆ ê³¼ ì¼€ì–´ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "flower_shop": "ê½ƒì§‘ì€ 'ê°ì„±', 'íŠ¹ë³„í•œ ë‚ ', 'ë§ˆìŒ ì „ë‹¬', 'ì•„ë¦„ë‹¤ì›€', 'ìì—°'ì„ ê°•ì¡°í•˜ì„¸ìš”. ê½ƒì˜ ì˜ë¯¸ì™€ ê°ë™ì„ ì‚´ë¦½ë‹ˆë‹¤.",
            "laundry": "ì„¸íƒì†ŒëŠ” 'ê¹¨ë—í•¨', 'í¸ë¦¬í•¨', 'ì‹ ë¢°', 'ì „ë¬¸ ì¼€ì–´'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ì˜·ì˜ ìˆ˜ëª… ì—°ì¥ê³¼ ì•ˆì‹¬ì„ ì „í•©ë‹ˆë‹¤.",

            # 11-20: ì¶”ê°€ ìƒí™œ ì„œë¹„ìŠ¤
            "convenience_store": "í¸ì˜ì ì€ 'í¸ë¦¬í•¨', '24ì‹œê°„', 'ë¹ ë¥¸ í•´ê²°', 'ì¼ìƒ í•„ìˆ˜'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ì–¸ì œë‚˜ ê°€ê¹Œìš´ ê³³ì—ì„œì˜ í¸ì˜ì„±ì„ ì „í•©ë‹ˆë‹¤.",
            "pharmacy": "ì•½êµ­ì€ 'ê±´ê°•', 'ì „ë¬¸ ìƒë‹´', 'ì‹ ë¢°', 'ì •í™•í•¨', 'ì¼€ì–´'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ê±´ê°• ì§€í‚´ì´ë¡œì„œì˜ ì „ë¬¸ì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤.",
            "hospital": "ë³‘ì›/ì˜ì›ì€ 'ê±´ê°•', 'ì „ë¬¸ ì§„ë£Œ', 'ì‹ ë¢°', 'ì •í™•í•œ ì§„ë‹¨', 'ì•ˆì‹¬'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì˜ë£Œ ì „ë¬¸ì„±ê³¼ í™˜ì ì¼€ì–´ë¥¼ ì „í•©ë‹ˆë‹¤.",
            "dental_clinic": "ì¹˜ê³¼ëŠ” 'ê±´ê°•í•œ ë¯¸ì†Œ', 'ì „ë¬¸ ì§„ë£Œ', 'ë¬´í†µ ì¹˜ë£Œ', 'ì˜ˆë°©', 'ìì‹ ê°'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì¹˜ì•„ ê±´ê°•ê³¼ ì‹¬ë¯¸ì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤.",
            "pet_shop": "ì• ì™„ë™ë¬¼ìƒµì€ 'ë°˜ë ¤ë™ë¬¼', 'ì‚¬ë‘', 'ì¼€ì–´', 'í–‰ë³µ', 'ê°€ì¡±'ì„ ê°•ì¡°í•˜ì„¸ìš”. ë°˜ë ¤ë™ë¬¼ê³¼ì˜ íŠ¹ë³„í•œ ìœ ëŒ€ê°ì„ ì‚´ë¦½ë‹ˆë‹¤.",
            "bookstore": "ì„œì ì€ 'ì§€ì‹', 'ì—¬ìœ ', 'íƒí—˜', 'ìƒˆë¡œìš´ ì„¸ê³„', 'íë§'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì±…ì„ í†µí•œ ê²½í—˜ê³¼ ì„±ì¥ì„ í‘œí˜„í•©ë‹ˆë‹¤.",
            "stationery": "ë¬¸êµ¬ì ì€ 'ì°½ì˜ì„±', 'í•™ìŠµ', 'ì¤€ë¹„', 'ì„¤ë ˜', 'ìƒˆ í•™ê¸°'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ë¬¸êµ¬ë¥¼ í†µí•œ ì¦ê±°ì›€ê³¼ ì¤€ë¹„ì„±ì„ ì „í•©ë‹ˆë‹¤.",
            "pc_cafe": "PCë°©ì€ 'ê²Œì„', 'ëª°ì…', 'í¸ì•ˆí•¨', 'ìµœì‹  ì‹œì„¤', 'ì¹œêµ¬ì™€ í•¨ê»˜'ë¥¼ ê°•ì¡°í•˜ì„¸ìš”. ê²Œì„ í™˜ê²½ê³¼ ì¦ê±°ì›€ì„ í‘œí˜„í•©ë‹ˆë‹¤.",
            "karaoke": "ë…¸ë˜ë°©ì€ 'ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ', 'ì¦ê±°ì›€', 'ì¶”ì–µ', 'ììœ ë¡œì›€', 'ì‹ ë‚˜ëŠ” ì‹œê°„'ì„ ê°•ì¡°í•˜ì„¸ìš”. ë…¸ë˜ë¥¼ í†µí•œ íë§ê³¼ ì¦ê±°ì›€ì„ ì „í•©ë‹ˆë‹¤.",
            "academy": "í•™ì›ì€ 'ì„±ì  í–¥ìƒ', 'ì „ë¬¸ ê°•ì‚¬', 'ì²´ê³„ì  í•™ìŠµ', 'ë¯¸ë˜ ì¤€ë¹„', 'ì„±ê³µ'ì„ ê°•ì¡°í•˜ì„¸ìš”. êµìœ¡ì˜ ì§ˆê³¼ ì„±ê³¼ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",

            # 21-30: ì „ë¬¸ ì„œë¹„ìŠ¤ ë° ê¸°íƒ€
            "yoga": "ìš”ê°€/í•„ë¼í…ŒìŠ¤ëŠ” 'ê· í˜•', 'ìœ ì—°ì„±', 'íë§', 'ê±´ê°•í•œ ëª¸ê³¼ ë§ˆìŒ', 'ëª…ìƒ'ì„ ê°•ì¡°í•˜ì„¸ìš”. ëª¸ê³¼ ë§ˆìŒì˜ ì¡°í™”ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "massage": "ë§ˆì‚¬ì§€/ìŠ¤íŒŒëŠ” 'íë§', 'ë¦´ë™ìŠ¤', 'í”¼ë¡œ íšŒë³µ', 'í”„ë¦¬ë¯¸ì—„ ì¼€ì–´', 'ì¬ì¶©ì „'ì„ ê°•ì¡°í•˜ì„¸ìš”. ê¹Šì€ íœ´ì‹ê³¼ íšŒë³µì„ ì „í•©ë‹ˆë‹¤.",
            "real_estate": "ë¶€ë™ì‚°ì€ 'ì‹ ë¢°', 'ì „ë¬¸ì„±', 'ìµœì ì˜ ì„ íƒ', 'ì •í™•í•œ ì •ë³´', 'ë‚´ ì§‘ ë§ˆë ¨'ì„ ê°•ì¡°í•˜ì„¸ìš”. ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œì˜ ì‹ ë¢°ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "car_wash": "ì„¸ì°¨ì¥ì€ 'ê¹¨ë—í•¨', 'ê´‘íƒ', 'í”„ë¦¬ë¯¸ì—„ ì¼€ì–´', 'ìƒˆì°¨ ê°™ì€ ëŠë‚Œ', 'í¸ë¦¬í•¨'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì°¨ëŸ‰ ê´€ë¦¬ì˜ ì™„ì„±ë„ë¥¼ ì „í•©ë‹ˆë‹¤.",
            "car_repair": "ìë™ì°¨ì •ë¹„ëŠ” 'ì•ˆì „', 'ì „ë¬¸ ê¸°ìˆ ', 'ì‹ ë¢°', 'ì •í™•í•œ ì§„ë‹¨', 'ì •ì§í•¨'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì°¨ëŸ‰ ì •ë¹„ì˜ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "optical_shop": "ì•ˆê²½ì ì€ 'ì‹œë ¥ ë³´í˜¸', 'ìŠ¤íƒ€ì¼', 'ì „ë¬¸ ê²€ì•ˆ', 'í¸ì•ˆí•¨', 'ëª…í’ˆ ì•ˆê²½'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì‹œë ¥ê³¼ ìŠ¤íƒ€ì¼ì˜ ì¡°í™”ë¥¼ ì „í•©ë‹ˆë‹¤.",
            "jewelry": "ì£¼ì–¼ë¦¬/ì•¡ì„¸ì„œë¦¬ëŠ” 'íŠ¹ë³„í•¨', 'ë¹›ë‚˜ëŠ” ìˆœê°„', 'í”„ë¦¬ë¯¸ì—„', 'ì„ ë¬¼', 'í’ˆê²©'ì„ ê°•ì¡°í•˜ì„¸ìš”. íŠ¹ë³„í•œ ìˆœê°„ì˜ ê°€ì¹˜ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "furniture": "ê°€êµ¬ì ì€ 'í¸ì•ˆí•¨', 'ì¸í…Œë¦¬ì–´', 'í’ˆì§ˆ', 'ë‚˜ë§Œì˜ ê³µê°„', 'ì‹¤ìš©ì„±'ì„ ê°•ì¡°í•˜ì„¸ìš”. ê³µê°„ì„ ì™„ì„±í•˜ëŠ” ê°€êµ¬ì˜ ê°€ì¹˜ë¥¼ ì „í•©ë‹ˆë‹¤.",
            "interior": "ì¸í…Œë¦¬ì–´ëŠ” 'ê³µê°„ ë³€ì‹ ', 'ë§ì¶¤ ë””ìì¸', 'ê°ê°', 'ì‹¤ìš©ì„±ê³¼ ë¯¸í•™', 'ê¿ˆì˜ ê³µê°„'ì„ ê°•ì¡°í•˜ì„¸ìš”. ê³µê°„ì˜ ì™„ì „í•œ ë³€í™”ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.",
            "cleaning_service": "ì²­ì†Œ/ì´ì‚¬ëŠ” 'ê¹¨ë—í•¨', 'í¸ë¦¬í•¨', 'ì „ë¬¸ì„±', 'ì‹ ë¢°', 'ìƒˆë¡œìš´ ì‹œì‘'ì„ ê°•ì¡°í•˜ì„¸ìš”. ì²­ê²°ê³¼ í¸ì˜ì„±ì„ ì „í•©ë‹ˆë‹¤.",
        }

        industry_guide = ""
        if industry and industry in industry_guides:
            industry_guide = f"\n\nì—…ì¢… íŠ¹í™” ê°€ì´ë“œë¼ì¸:\n{industry_guides[industry]}"

        history_context = self._build_history_context(
            chat_history=chat_history,
            generation_history=generation_history,
        )
        history_section = f"\n\n{history_context}" if history_context else ""

        return f"{base_prompt}\n\ní†¤ ì•¤ ë§¤ë„ˆ:\n{tone_guide}{industry_guide}{history_section}"

    def _build_user_prompt(
        self,
        user_input,
        max_length,
        min_length=None,
        history_context=None,
        hashtag_count=None,
    ):
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        history_note = ""
        if history_context:
            history_note = f"\n\n{history_context}"

        requirements = [f"ë°˜ë“œì‹œ {max_length}ì ì´ë‚´ (ì´ˆê³¼ ê¸ˆì§€)"]
        if min_length is not None:
            requirements.append(f"ìµœì†Œ {min_length}ì ì´ìƒ")
        if hashtag_count is not None:
            requirements.append(
                f"í•´ì‹œíƒœê·¸ {hashtag_count}ê°œë¥¼ ë¬¸ì¥ ëì— í¬í•¨ (ê° í•´ì‹œíƒœê·¸ëŠ” #ìœ¼ë¡œ ì‹œì‘, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)"
            )
        requirements_text = "\n- ".join(requirements)

        return f"""ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ ê´‘ê³  ë¬¸êµ¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”:

{user_input}{history_note}

ìš”êµ¬ì‚¬í•­:
- {requirements_text}
- ê´‘ê³  ë¬¸êµ¬ë§Œ ì‘ì„± (ì„¤ëª…, ë²ˆí˜¸ ë“± ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œì™¸)
- ê°ì„±ì ì´ë©´ì„œë„ ëª…í™•í•œ ë©”ì‹œì§€ ì „ë‹¬

ê´‘ê³  ë¬¸êµ¬:"""

    def _postprocess(self, text, max_length):
        """í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""

        # 1. ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = text.replace("1. ", "").replace("2. ", "").replace("- ", "")
        text = text.replace('"', '').replace("'", "").replace('ã€Œ', '').replace('ã€', '')
        text = text.strip()

        # 2. ê¸¸ì´ ì œí•œ
        if len(text) > max_length:
            trimmed = text[:max_length]
            last_space = trimmed.rfind(" ")
            if last_space >= int(max_length * 0.6):
                trimmed = trimmed[:last_space]
            trimmed = trimmed.strip()
            logger.debug(
                "postprocess: trimmed from %s to %s chars",
                len(text),
                len(trimmed),
            )
            text = trimmed

        # 3. ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if not text:
            return self._get_fallback_copy()

        return text

    def _get_fallback_copy(self):
        """GPT ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸êµ¬ ë°˜í™˜"""
        return "íŠ¹ë³„í•œ ìˆœê°„ì„ í•¨ê»˜í•˜ì„¸ìš”"

    def _append_length_emphasis(self, prompt: str, max_length: int, min_length: int) -> str:
        return (
            f"{prompt}\n\nì¤‘ìš”: ë°˜ë“œì‹œ {max_length}ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. "
            f"ë„ˆë¬´ ì§§ê²Œ ì“°ì§€ ë§ê³  ìµœì†Œ {min_length}ì ì´ìƒì„ ì§€í‚¤ì„¸ìš”."
        )

    def _build_history_context(self, chat_history=None, generation_history=None) -> str:
        sections = []
        chat_part = self._format_chat_history(chat_history)
        if chat_part:
            sections.append(f"ìµœê·¼ ëŒ€í™” ë§¥ë½(ì°¸ê³ ):\n{chat_part}")
        gen_part = self._format_generation_history(generation_history)
        if gen_part:
            sections.append(f"ì´ì „ì— ìƒì„±ëœ ë¬¸êµ¬(ì°¸ê³ ):\n{gen_part}")
        return "\n\n".join(sections)

    def _append_hashtag_instruction(self, prompt: str, hashtag_count: int) -> str:
        return (
            f"{prompt}\n\ní•´ì‹œíƒœê·¸ ê·œì¹™: ë¬¸ì¥ ëì— í•´ì‹œíƒœê·¸ {hashtag_count}ê°œë¥¼ í¬í•¨í•˜ê³ , "
            "ê° í•´ì‹œíƒœê·¸ëŠ” #ìœ¼ë¡œ ì‹œì‘í•˜ë©° ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”."
        )

    def _append_user_requirements(
        self,
        user_prompt: str,
        max_length: int,
        min_length: int,
        hashtag_count: Optional[int],
    ) -> str:
        requirements = [
            f"- ë°˜ë“œì‹œ {max_length}ì ì´ë‚´ (ì´ˆê³¼ ê¸ˆì§€)",
            f"- ìµœì†Œ {min_length}ì ì´ìƒ",
        ]
        if hashtag_count:
            requirements.append(
                f"- ë¬¸ì¥ ëì— í•´ì‹œíƒœê·¸ {hashtag_count}ê°œ í¬í•¨ (#ìœ¼ë¡œ ì‹œì‘, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)"
            )
        return f"{user_prompt}\n\nìš”êµ¬ì‚¬í•­:\n" + "\n".join(requirements)

    def _detect_hashtag_requirement(self, user_input, chat_history=None) -> tuple[bool, int]:
        combined = " ".join(
            [user_input or ""]
            + [msg.get("content", "") for msg in (chat_history or [])[-5:]]
        )
        text = combined.lower()
        if "í•´ì‹œíƒœê·¸" in text or "hash tag" in text or "hashtag" in text or "#" in text:
            count = self._extract_hashtag_count(combined)
            return True, count
        return False, 0

    def _extract_hashtag_count(self, text: str) -> int:
        match = re.search(r"í•´ì‹œ\s*íƒœê·¸\s*(\d+)\s*ê°œ|í•´ì‹œíƒœê·¸\s*(\d+)\s*ê°œ|í•´ì‹œíƒœê·¸\s*(\d+)", text)
        if match:
            for group in match.groups():
                if group and group.isdigit():
                    value = int(group)
                    return max(1, min(value, 10))
        return 3

    def _extract_length_expectation(self, user_input: str) -> "LengthSpec":
        if not user_input:
            return LengthSpec(None, None, "none", False)

        text = user_input.replace(" ", "")

        range_match = re.search(r"(\d{2,4})[~\-](\d{2,4})ì", text)
        if range_match:
            start = int(range_match.group(1))
            end = int(range_match.group(2))
            if start > end:
                start, end = end, start
            return LengthSpec(start, end, "range", end >= 400)

        approx_match = re.search(r"ì•½(\d{2,4})ì", text)
        if approx_match:
            value = int(approx_match.group(1))
            return LengthSpec(int(value * 0.8), value, "approx", value >= 400)

        exact_match = re.search(r"(\d{2,4})ì", text)
        if exact_match:
            value = int(exact_match.group(1))
            return LengthSpec(int(value * 0.8), value, "exact", value >= 400)

        if "ê¸¸ê²Œ" in text:
            return LengthSpec(400, 600, "long", True)
        if "ì§§ê²Œ" in text:
            return LengthSpec(20, 40, "short", False)

        return LengthSpec(None, None, "none", False)

    def _merge_length_source_text(self, user_input) -> str:
        return user_input or ""

    def _format_chat_history(self, chat_history, max_items: int = 5, max_len: int = 120) -> str:
        if not chat_history:
            return ""
        lines = []
        for msg in chat_history[-max_items:]:
            role = msg.get("role", "unknown")
            content = " ".join((msg.get("content") or "").split())
            if not content:
                continue
            snippet = content[:max_len]
            if len(content) > max_len:
                snippet += "..."
            lines.append(f"- {role}: {snippet}")
        return "\n".join(lines)

    def _format_generation_history(
        self,
        generation_history,
        max_items: int = 3,
        max_len: int = 120,
    ) -> str:
        if not generation_history:
            return ""
        lines = []
        for gen in generation_history:
            if gen.get("content_type") != "text":
                continue
            output_text = " ".join((gen.get("output_text") or "").split())
            if not output_text:
                continue
            snippet = output_text[:max_len]
            if len(output_text) > max_len:
                snippet += "..."
            lines.append(f"- {snippet}")
            if len(lines) >= max_items:
                break
        return "\n".join(lines)


class LengthSpec:
    def __init__(self, min_len: Optional[int], max_len: Optional[int], source: str, is_long: bool):
        self.min_len = min_len
        self.max_len = max_len
        self.source = source
        self.is_long = is_long


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ“ TextGenerator í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    generator = TextGenerator()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "input": "ì¹´í˜ ì‹ ë©”ë‰´ í™ë³´, ë”°ëœ»í•œ ëŠë‚Œ, ê²¨ìš¸ ì‹œì¦Œ",
            "tone": "warm"
        },
        {
            "input": "ì‹ë‹¹ ê°€ì¡± ëª¨ì„ ì´ë²¤íŠ¸, ì£¼ë§ íŠ¹ê°€",
            "tone": "friendly"
        },
        {
            "input": "í—¬ìŠ¤ì¥ ì‹ ê·œ íšŒì› ëª¨ì§‘, ì „ë¬¸ íŠ¸ë ˆì´ë„ˆ",
            "tone": "professional"
        }
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"í…ŒìŠ¤íŠ¸ {i}")
        print(f"{'='*60}")

        result = generator.generate_ad_copy(
            user_input=test["input"],
            tone=test["tone"]
        )

        print(f"\nê²°ê³¼: '{result}'")
        print(f"ê¸¸ì´: {len(result)}ì")

    print(f"\n{'='*60}")
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"{'='*60}")
