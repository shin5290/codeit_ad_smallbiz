"""
Civitai 25ê°œ í”„ë¡¬í”„íŠ¸ ì‹¬í™” ë¶„ì„ ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ v2.0

ìƒˆë¡œ ì¶”ê°€ëœ í‰ê°€ í•­ëª©:
- ìš”ì†Œ ìˆœì„œ ì¼ê´€ì„± (Medium â†’ Lighting â†’ Technical)
- ë Œì¦ˆ ì¤‘ë³µ ì²´í¬
- ìì—°ì–´ ë¹„ìœ¨ (15% ìµœì )
"""

import re
from typing import Dict, Tuple, List
from dataclasses import dataclass


# ============================================
# í‰ê°€ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# ============================================

@dataclass
class EvaluationResult:
    civitai_compliance: Dict[str, bool]
    compliance_rate: float
    quality_metrics: Dict[str, float]
    avg_quality: float
    details: Dict
    warnings: List[str]


# ============================================
# Evaluator ë³¸ì²´
# ============================================

class CivitaiEnhancedEvaluator:
    """
    Civitai 25ê°œ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ
    """

    BENCHMARK_STATS = {
        "word_count": {"optimal_range": (26, 41)},
        "comma_count": {"optimal_range": (6, 9)},
        "natural_language_ratio": {"optimal_range": (10, 20)},
        "keyword_repetition": {"optimal_range": (0, 2)},
        "photo_terms": {
            "lighting": 0.96,
            "lens": 0.80,
            "composition": 0.68,
            "aperture": 0.52,
        }
    }

    NATURAL_LANGUAGE_PATTERNS = [
        r'(Professional|Commercial|Editorial)\s+\w+\s+photography\s+of',
        r'\w+\s+(on|in|at|with|during|featuring)\s+',
        r'\w+ing\s+\w+',
    ]

    QUALITY_SPAM = [
        'masterpiece', 'best quality', 'high quality',
        'ultra detailed', 'highly detailed',
        '8k', '4k', '16k', 'award-winning'
    ]

    # ============================================
    # 1ë‹¨ê³„: Civitai ê¶Œì¥ì‚¬í•­
    # ============================================

    def check_civitai_compliance(self, prompt: str, negative: str) -> Dict[str, bool]:
        checks = {}

        # ìì—°ì–´ êµ¬ë¬¸
        nl_count = sum(1 for p in self.NATURAL_LANGUAGE_PATTERNS if re.search(p, prompt))
        checks['ìì—°ì–´_êµ¬ë¬¸'] = nl_count >= 1

        # Quality spam
        prompt_lower = prompt.lower()
        checks['quality_spam_ì œê±°'] = not any(spam in prompt_lower for spam in self.QUALITY_SPAM)

        # Negative ìµœì†Œí™”
        neg_keywords = [k.strip() for k in negative.split(',') if k.strip()]
        checks['negative_ìµœì†Œí™”'] = 5 <= len(neg_keywords) <= 7

        # ì‚¬ì§„ ìš©ì–´
        has_lighting = any(t in prompt_lower for t in ['light', 'lighting'])
        has_lens = bool(re.search(r'\d+mm', prompt_lower)) or 'lens' in prompt_lower
        checks['ì‚¬ì§„_ìš©ì–´'] = has_lighting or has_lens

        return checks

    # ============================================
    # 2ë‹¨ê³„: í’ˆì§ˆ ì§€í‘œ
    # ============================================

    def calculate_quality_metrics(self, prompt: str) -> Tuple[Dict[str, float], List[str]]:
        metrics = {}
        warnings = []

        # ê¸¸ì´
        wc = len(prompt.split())
        lo, hi = self.BENCHMARK_STATS['word_count']['optimal_range']
        metrics['ê¸¸ì´'] = 100.0 if lo <= wc <= hi else max(0, 100 - abs(wc - (lo if wc < lo else hi)) * 5)
        if wc < lo:
            warnings.append(f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({wc} < {lo})")
        elif wc > hi:
            warnings.append(f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹€ ({wc} > {hi})")

        # í‚¤ì›Œë“œ ë°¸ëŸ°ìŠ¤
        commas = prompt.count(',')
        lo, hi = self.BENCHMARK_STATS['comma_count']['optimal_range']
        metrics['í‚¤ì›Œë“œ_ë°¸ëŸ°ìŠ¤'] = 100.0 if lo <= commas <= hi else max(0, 100 - abs(commas - (lo if commas < lo else hi)) * 10)

        # ìì—°ì–´ ë¹„ìœ¨
        nl_ratio = self._calculate_nl_ratio(prompt)
        lo, hi = self.BENCHMARK_STATS['natural_language_ratio']['optimal_range']
        if lo <= nl_ratio <= hi:
            metrics['ìì—°ì–´_ë¹„ìœ¨'] = 100.0
        elif nl_ratio < lo:
            metrics['ìì—°ì–´_ë¹„ìœ¨'] = (nl_ratio / lo) * 100
            warnings.append(f"ìì—°ì–´ êµ¬ë¬¸ ë¶€ì¡± ({nl_ratio:.0f}% < {lo}%)")
        else:
            metrics['ìì—°ì–´_ë¹„ìœ¨'] = max(0, 100 - (nl_ratio - hi) * 5)
            warnings.append(f"ìì—°ì–´ ë¹„ìœ¨ ê³¼ë‹¤ ({nl_ratio:.0f}% > {hi}%)")

        # ìš”ì†Œ ìˆœì„œ
        score, w = self._check_element_order(prompt)
        metrics['ìš”ì†Œ_ìˆœì„œ'] = score
        warnings.extend(w)

        # ë Œì¦ˆ ì¤‘ë³µ
        score, w = self._check_lens_duplication(prompt)
        metrics['ë Œì¦ˆ_ì‚¬ì–‘'] = score
        warnings.extend(w)

        # ì‚¬ì§„ ìš©ì–´ ì™„ì„±ë„
        metrics['ì‚¬ì§„_ìš©ì–´_ì™„ì„±ë„'] = self._calculate_photo_completeness(prompt)

        # í‚¤ì›Œë“œ ì¤‘ë³µ
        metrics['í‚¤ì›Œë“œ_ì¤‘ë³µ_ìµœì†Œí™”'] = self._check_keyword_repetition(prompt)

        return metrics, warnings

    # ============================================
    # ì„¸ë¶€ ê³„ì‚° í•¨ìˆ˜ë“¤
    # ============================================

    def _calculate_nl_ratio(self, prompt: str) -> float:
        preps = ['on', 'in', 'at', 'with', 'of', 'from', 'during', 'by']
        verbs = re.findall(r'\w+ing\b', prompt.lower())
        count = sum(1 for p in preps if f' {p} ' in f' {prompt.lower()} ') + len(verbs)
        return (count / len(prompt.split())) * 100 if prompt.split() else 0

    def _check_element_order(self, prompt: str) -> Tuple[float, List[str]]:
        warnings = []
        words = prompt.lower().split()
        total = len(words)

        def pos(keys):
            for i, w in enumerate(words):
                if any(k in w for k in keys):
                    return (i / total) * 100
            return None

        medium = pos(['photograph'])
        lighting = pos(['light', 'lighting', 'lit'])
        technical = pos(['mm', 'lens', 'f/'])

        score = 100.0
        if medium and lighting and medium > lighting:
            score -= 20
            warnings.append("Mediumì´ Lightingë³´ë‹¤ ë’¤ì— ìœ„ì¹˜")
        if lighting and technical and lighting > technical:
            score -= 20
            warnings.append("Lightingì´ Technicalë³´ë‹¤ ë’¤ì— ìœ„ì¹˜")

        return max(0, score), warnings

    def _check_lens_duplication(self, prompt: str) -> Tuple[float, List[str]]:
        lenses = re.findall(r'\d+mm', prompt)
        if len(lenses) > 1:
            return 0.0, [f"ì—¬ëŸ¬ ë Œì¦ˆ ëª…ì‹œë¨: {', '.join(lenses)}"]
        return 100.0, []

    def _calculate_photo_completeness(self, prompt: str) -> float:
        p = prompt.lower()
        terms = {
            'lens': bool(re.search(r'\d+mm', p) or 'lens' in p),
            'aperture': bool(re.search(r'f/\d', p)),
            'lighting': any(t in p for t in ['light', 'lighting']),
            'composition': any(t in p for t in ['shot', 'angle', 'focus', 'depth']),
        }
        score = 0
        for k, v in terms.items():
            if v and self.BENCHMARK_STATS['photo_terms'].get(k, 0) >= 0.7:
                score += 25
        return score

    def _check_keyword_repetition(self, prompt: str) -> float:
        words = [w.lower().strip(',.') for w in prompt.split() if len(w) > 3]
        repeated = sum(1 for w in set(words) if words.count(w) > 1)
        lo, hi = self.BENCHMARK_STATS['keyword_repetition']['optimal_range']
        return 100.0 if repeated <= hi else max(0, 100 - (repeated - hi) * 20)

    # ============================================
    # ì¢…í•© í‰ê°€
    # ============================================

    def evaluate(self, prompt: str, negative: str, industry=None, config=None) -> EvaluationResult:
        config = config or {}

        compliance = self.check_civitai_compliance(prompt, negative)
        compliance_rate = sum(compliance.values()) / len(compliance)

        metrics, warnings = self.calculate_quality_metrics(prompt)
        avg_quality = sum(metrics.values()) / len(metrics)

        details = {
            "word_count": len(prompt.split()),
            "comma_count": prompt.count(','),
            "negative_count": len([k for k in negative.split(',') if k.strip()]),
            "nl_ratio": self._calculate_nl_ratio(prompt)
        }

        return EvaluationResult(
            civitai_compliance=compliance,
            compliance_rate=compliance_rate,
            quality_metrics=metrics,
            avg_quality=avg_quality,
            details=details,
            warnings=warnings
        )

from datetime import datetime
from pathlib import Path


def save_md_report(
    result,
    prompt: str,
    negative: str,
    industry: str | None = None,
    output_dir: str = "reports"
):
    """
    EvaluationResult â†’ Markdown ë¦¬í¬íŠ¸ ì €ì¥
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"prompt_evaluation_{timestamp}.md"
    filepath = Path(output_dir) / filename

    lines = []

    # =========================
    # Header
    # =========================
    lines.append("# Prompt Evaluation Report")
    lines.append("")
    lines.append(f"- **Generated at**: {timestamp}")
    if industry:
        lines.append(f"- **Industry**: {industry}")
    lines.append("")

    # =========================
    # Prompt
    # =========================
    lines.append("## Prompt")
    lines.append("```")
    lines.append(prompt.strip())
    lines.append("```")
    lines.append("")

    lines.append("## Negative Prompt")
    lines.append("```")
    lines.append(negative.strip())
    lines.append("```")
    lines.append("")

    # =========================
    # Compliance
    # =========================
    lines.append("## Civitai Compliance")
    lines.append("")
    lines.append(f"- **Compliance Rate**: {result.compliance_rate * 100:.0f}%")
    lines.append("")
    for k, v in result.civitai_compliance.items():
        status = "âœ…" if v else "âŒ"
        lines.append(f"- {status} {k}")
    lines.append("")

    # =========================
    # Quality Metrics
    # =========================
    lines.append("## Quality Metrics (Benchmark-based)")
    lines.append("")
    lines.append(f"- **Average Quality Score**: {result.avg_quality:.1f}%")
    lines.append("")

    for metric, score in result.quality_metrics.items():
        lines.append(f"- **{metric}**: {score:.1f}%")
    lines.append("")

    # =========================
    # Details
    # =========================
    lines.append("## Details")
    lines.append("")
    for k, v in result.details.items():
        if isinstance(v, float):
            lines.append(f"- **{k}**: {v:.2f}")
        else:
            lines.append(f"- **{k}**: {v}")
    lines.append("")

    # =========================
    # Warnings
    # =========================
    if result.warnings:
        lines.append("## Warnings")
        lines.append("")
        for w in result.warnings:
            lines.append(f"- âš ï¸ {w}")
        lines.append("")
    else:
        lines.append("## Warnings")
        lines.append("")
        lines.append("- None ğŸ‰")
        lines.append("")

    # =========================
    # Save
    # =========================
    filepath.write_text("\n".join(lines), encoding="utf-8")

    return filepath



# ============================================
# í…ŒìŠ¤íŠ¸
# ============================================

if __name__ == "__main__":
    evaluator = CivitaiEnhancedEvaluator()

    prompt = """Professional food photography of strawberry latte on marble table,
    minimalist cafe interior with natural light,
    soft window lighting from left,
    overhead shot, warm pastel tones,
    85mm lens, f/1.8 aperture"""

    negative = "cartoon, illustration, painting, low quality, blurry, artificial"

    result = evaluator.evaluate(prompt, negative)

    print("ì¤€ìˆ˜ìœ¨:", f"{result.compliance_rate*100:.0f}%")
    print("í‰ê·  í’ˆì§ˆ:", f"{result.avg_quality:.1f}%")
    print("ê²½ê³ :", result.warnings)
