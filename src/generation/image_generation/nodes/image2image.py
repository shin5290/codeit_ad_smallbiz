"""
Image-to-Image Generation Node (Z-Image Turbo)
Z-Image Turbo I2Ië¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ë³€í™˜

íŠ¹ì§•:
- ë¹ ë¥¸ ì†ë„ (8 steps)
- ì›ë³¸ êµ¬ë„ ìœ ì§€í•˜ë©´ì„œ ìŠ¤íƒ€ì¼ ë³€í™˜
- ì œí’ˆ ì‚¬ì§„ â†’ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼
- ì¼ë°˜ ì‚¬ì§„ â†’ ì´ˆì‚¬ì‹¤ì  ìŠ¤íƒ€ì¼
"""

import os
import gc
import threading
from typing import Dict, Any, Optional
from pathlib import Path

import torch
from PIL import Image
from diffusers import (
    ZImageImg2ImgPipeline,
    FlowMatchEulerDiscreteScheduler,
)

from .base import BaseNode
from ..config import aspect_ratio_templates

# Z-Image Turbo ëª¨ë¸ ê²½ë¡œ
ZIT_MODELS_DIR = Path(os.getenv("ZIT_MODELS_DIR", "/opt/ai-models/zit"))
ZIT_BASE_MODEL = ZIT_MODELS_DIR / "Z-Image-Turbo-BF16"
ZIT_LORA_DIR = ZIT_MODELS_DIR / "lora"

# ==============================================================================
# â˜… ì „ì—­ ìƒíƒœ ê´€ë¦¬ (Text2ImageNodeì™€ ê³µìœ  ê°€ëŠ¥)
# ==============================================================================
_GLOBAL_I2I_PIPE = None
_EXECUTION_LOCK = threading.Lock()
_EXECUTION_COUNT = 0


class Image2ImageNode(BaseNode):
    """
    Z-Image Turbo Image-to-Image ë…¸ë“œ

    ì‚¬ìš© ì¼€ì´ìŠ¤:
    1. ìŠ¤íƒ€ì¼ ë³€í™˜: ì‚¬ì‹¤ì  ì œí’ˆ ì‚¬ì§„ â†’ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼
    2. í’ˆì§ˆ í–¥ìƒ: ì¼ë°˜ ì‚¬ì§„ â†’ ì´ˆì‚¬ì‹¤ì  ê³ í’ˆì§ˆ ì‚¬ì§„
    3. êµ¬ë„ ìœ ì§€ ì¬ìƒì„±: ìŠ¤ì¼€ì¹˜ â†’ ì™„ì„±ëœ ê·¸ë¦¼

    Example:
        node = Image2ImageNode()
        result = node.execute({
            "prompt": "anime style, vibrant colors, illustrated product",
            "reference_image": product_photo,  # PIL.Image
            "strength": 0.6,  # ë³€í˜• ê°•ë„ (0.3~0.7 ê¶Œì¥)
            "aspect_ratio": "1:1",
            "num_inference_steps": 8,
            "seed": 42
        })
        image = result["image"]
    """

    def __init__(self, device: Optional[str] = None, auto_unload: bool = False):
        super().__init__("Image2ImageNode")
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.auto_unload = auto_unload

    def load_pipeline(self):
        """ZImageImg2ImgPipeline ë¡œë“œ (ì „ì—­ ìºì‹œ)"""
        global _GLOBAL_I2I_PIPE

        if _GLOBAL_I2I_PIPE is not None:
            return _GLOBAL_I2I_PIPE

        print(f"[{self.node_name}] ğŸš€ Loading ZIT I2I Pipeline (20.5GB)...")
        try:
            scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
                str(ZIT_BASE_MODEL), subfolder="scheduler"
            )

            pipe = ZImageImg2ImgPipeline.from_pretrained(
                str(ZIT_BASE_MODEL),
                scheduler=scheduler,
                torch_dtype=torch.bfloat16,
                local_files_only=True,
                low_cpu_mem_usage=True
            )

            # ì „ì²´ ëª¨ë¸ì„ GPUë¡œ ì´ë™ (20.5GB < 23GB VRAM)
            pipe.to(self.device)
            print(f"[{self.node_name}] Pipeline moved to {self.device}")

            # Attention ìµœì í™”
            try:
                if hasattr(pipe, "transformer") and hasattr(pipe.transformer, "set_attn_processor"):
                    from diffusers.models.attention_processor import AttnProcessor2_0
                    pipe.transformer.set_attn_processor(AttnProcessor2_0())
                    print(f"[{self.node_name}] FlashAttention enabled")
            except Exception as e:
                print(f"[{self.node_name}] Could not enable attention optimization: {e}")

            # VAE ìµœì í™”
            pipe.vae.enable_tiling()
            pipe.vae.enable_slicing()
            print(f"[{self.node_name}] VAE tiling/slicing enabled")

            print(f"[{self.node_name}] âœ… I2I Pipeline loaded successfully")

            _GLOBAL_I2I_PIPE = pipe
            return pipe

        except Exception as e:
            raise RuntimeError(f"I2I Pipeline Load Failed: {e}")

    def get_generator_device(self, pipe):
        """Generator ë””ë°”ì´ìŠ¤ ê²°ì • (CPU offload ê³ ë ¤)"""
        if hasattr(pipe, "_execution_device"):
            return pipe._execution_device
        return self.device

    def process(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """I2I ì´ë¯¸ì§€ ìƒì„±"""
        global _EXECUTION_COUNT

        with _EXECUTION_LOCK:
            # ì…ë ¥ ì¶”ì¶œ
            prompt = inputs.get("prompt", "")
            reference_image = inputs.get("reference_image")  # PIL.Image
            strength = inputs.get("strength", 0.6)  # ê¸°ë³¸ê°’ 0.6
            aspect_ratio = inputs.get("aspect_ratio", "1:1")
            num_inference_steps = inputs.get("num_inference_steps", 8)
            seed = inputs.get("seed", None)

            # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
            if reference_image is None:
                raise ValueError("reference_image is required for Image2Image generation")

            if not isinstance(reference_image, Image.Image):
                raise ValueError("reference_image must be a PIL.Image.Image object")

            if reference_image.mode != "RGB":
                reference_image = reference_image.convert("RGB")

            # í•´ìƒë„ ê²°ì •
            width, height = aspect_ratio_templates.get_size(aspect_ratio)

            # ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            reference_image = reference_image.resize((width, height), Image.Resampling.LANCZOS)

            print(f"[{self.node_name}] Input: {width}x{height}, strength={strength}")

            # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            pipe = self.load_pipeline()

            # ì œë„ˆë ˆì´í„° ìƒì„± ë° ì‹œë“œ ì¶”ì¶œ
            exec_device = self.get_generator_device(pipe)

            if seed is None:
                import random
                seed = random.randint(0, 2**32 - 1)

            generator = torch.Generator(device=exec_device).manual_seed(seed)

            print(f"[{self.node_name}] Generating I2I ({width}x{height}, seed={seed}, strength={strength})...")

            # I2I ìƒì„±
            with torch.no_grad():
                image = pipe(
                    prompt=prompt,
                    image=reference_image,
                    strength=strength,  # ë³€í˜• ê°•ë„ (0.0~1.0)
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=0.0,  # ZITëŠ” CFG ë¯¸ì‚¬ìš©
                    generator=generator,
                    output_type="pil"
                ).images[0]

            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (5íšŒë§ˆë‹¤)
            _EXECUTION_COUNT += 1
            if _EXECUTION_COUNT % 5 == 0:
                print(f"[{self.node_name}] ğŸ§¹ Periodic Memory Cleanup (Count: {_EXECUTION_COUNT})")
                gc.collect()
                torch.cuda.empty_cache()

            # auto_unloadê°€ Trueì¼ ë•Œë§Œ ê°•ì œ ì¢…ë£Œ
            if self.auto_unload:
                self.flush_global()

            return {"image": image, "seed": seed, "width": width, "height": height}

    def flush_global(self):
        """ì „ì—­ ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        global _GLOBAL_I2I_PIPE
        if _GLOBAL_I2I_PIPE is not None:
            del _GLOBAL_I2I_PIPE
            _GLOBAL_I2I_PIPE = None
        gc.collect()
        torch.cuda.empty_cache()
        print(f"[{self.node_name}] I2I Pipeline flushed")

    def get_required_inputs(self):
        return ['prompt', 'reference_image']

    def get_input_keys(self):
        return ["prompt", "reference_image", "strength", "aspect_ratio", "num_inference_steps", "seed"]

    def get_output_keys(self):
        return ["image", "seed", "width", "height"]
