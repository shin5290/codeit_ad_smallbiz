"""
Image-to-Image Generation Node (Z-Image Turbo)
Z-Image Turbo I2Ië¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ë³€í™˜

íŠ¹ì§•:
- ë¹ ë¥¸ ì†ë„ (8 steps)
- ì›ë³¸ êµ¬ë„ ìœ ì§€í•˜ë©´ì„œ ìŠ¤íƒ€ì¼ ë³€í™˜
- ì œí’ˆ ì‚¬ì§„ â†’ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼
- ì¼ë°˜ ì‚¬ì§„ â†’ ì´ˆì‚¬ì‹¤ì  ìŠ¤íƒ€ì¼
"""

import gc
import threading
from typing import Dict, Any, Optional

import torch
from PIL import Image

from src.utils.logging import get_logger

from .base import BaseNode
from ..config import aspect_ratio_templates
from ..shared_cache import get_i2i_pipeline, flush_shared_cache

logger = get_logger(__name__)

# ==============================================================================
# â˜… ì „ì—­ ìƒíƒœ ê´€ë¦¬ (ê³µìœ  ìºì‹œ ì‚¬ìš©)
# ==============================================================================
_EXECUTION_LOCK = threading.Lock()
_EXECUTION_COUNT = 0


class Image2ImageNode(BaseNode):
    """
    Z-Image Turbo Image-to-Image ë…¸ë“œ

    ì‚¬ìš© ì¼€ì´ìŠ¤:
    1. ìŠ¤íƒ€ì¼ ë³€í™˜: ì‚¬ì‹¤ì  ì œí’ˆ ì‚¬ì§„ â†’ ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼
    2. í’ˆì§ˆ í–¥ìƒ: ì¼ë°˜ ì‚¬ì§„ â†’ ì´ˆì‚¬ì‹¤ì  ê³ í’ˆì§ˆ ì‚¬ì§„
    3. êµ¬ë„ ìœ ì§€ ì¬ìƒì„±: ìŠ¤ì¼€ì¹˜ â†’ ì™„ì„±ëœ ê·¸ë¦¼

    Example:
        node = Image2ImageNode()
        result = node.execute({
            "prompt": "anime style, vibrant colors, illustrated product",
            "reference_image": product_photo,  # PIL.Image
            "strength": 0.6,  # ë³€í˜• ê°•ë„ (0.3~0.7 ê¶Œì¥)
            "aspect_ratio": "1:1",
            "num_inference_steps": 8,
            "seed": 42
        })
        image = result["image"]
    """

    def __init__(self, device: Optional[str] = None, auto_unload: bool = False):
        super().__init__("Image2ImageNode")
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.auto_unload = auto_unload

    def load_pipeline(self):
        """ê³µìœ  ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ I2I íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        logger.info(f"[{self.node_name}] Loading I2I pipeline (shared cache)...")
        return get_i2i_pipeline(self.device)

    def get_generator_device(self, pipe):
        """Generator ë””ë°”ì´ìŠ¤ ê²°ì • (CPU offload ê³ ë ¤)"""
        if hasattr(pipe, "_execution_device"):
            return pipe._execution_device
        return self.device

    def process(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """I2I ì´ë¯¸ì§€ ìƒì„±"""
        global _EXECUTION_COUNT

        with _EXECUTION_LOCK:
            # ì…ë ¥ ì¶”ì¶œ
            prompt = inputs.get("prompt", "")
            reference_image = inputs.get("reference_image")  # PIL.Image
            strength = inputs.get("strength", 0.6)  # ê¸°ë³¸ê°’ 0.6
            aspect_ratio = inputs.get("aspect_ratio", "1:1")
            num_inference_steps = inputs.get("num_inference_steps", 8)
            seed = inputs.get("seed", None)

            # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
            if reference_image is None:
                raise ValueError("reference_image is required for Image2Image generation")

            if not isinstance(reference_image, Image.Image):
                raise ValueError("reference_image must be a PIL.Image.Image object")

            if reference_image.mode != "RGB":
                reference_image = reference_image.convert("RGB")

            # í•´ìƒë„ ê²°ì •
            width, height = aspect_ratio_templates.get_size(aspect_ratio)

            # ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            reference_image = reference_image.resize((width, height), Image.Resampling.LANCZOS)

            logger.info(f"[{self.node_name}] Input: {width}x{height}, strength={strength}")

            # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            pipe = self.load_pipeline()

            # ì œë„ˆë ˆì´í„° ìƒì„± ë° ì‹œë“œ ì¶”ì¶œ
            exec_device = self.get_generator_device(pipe)

            if seed is None:
                import random
                seed = random.randint(0, 2**32 - 1)

            generator = torch.Generator(device=exec_device).manual_seed(seed)

            logger.info(f"[{self.node_name}] Generating I2I ({width}x{height}, seed={seed}, strength={strength})...")

            # I2I ìƒì„±
            with torch.no_grad():
                image = pipe(
                    prompt=prompt,
                    image=reference_image,
                    strength=strength,  # ë³€í˜• ê°•ë„ (0.0~1.0)
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=0.0,  # ZITëŠ” CFG ë¯¸ì‚¬ìš©
                    generator=generator,
                    output_type="pil"
                ).images[0]

            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (5íšŒë§ˆë‹¤)
            _EXECUTION_COUNT += 1
            if _EXECUTION_COUNT % 5 == 0:
                logger.info(f"[{self.node_name}] ğŸ§¹ Periodic Memory Cleanup (Count: {_EXECUTION_COUNT})")
                gc.collect()
                torch.cuda.empty_cache()

            # auto_unloadê°€ Trueì¼ ë•Œë§Œ ê°•ì œ ì¢…ë£Œ
            if self.auto_unload:
                self.flush_global()

            return {"image": image, "seed": seed, "width": width, "height": height}

    def flush_global(self):
        """ì „ì—­ ìºì‹œ ì™„ì „ ì´ˆê¸°í™” (ê³µìœ  ìºì‹œ ì‚¬ìš©)"""
        flush_shared_cache()

    def get_required_inputs(self):
        return ['prompt', 'reference_image']

    def get_output_keys(self):
        return ["image", "seed", "width", "height"]
