"""
Z-Image Turbo Text2Image Node
Z-Image Turbo ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ ì† ì´ë¯¸ì§€ ìƒì„± ë…¸ë“œ

íŠ¹ì§•:
- 8 stepsë¡œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± (~1-2ì´ˆ)
- ê¸´ í”„ë¡¬í”„íŠ¸ ì§€ì› (CLIP 77 í† í° ì œí•œ ì—†ìŒ, T5 ê¸°ë°˜)
- LoRAë¥¼ í†µí•œ ìŠ¤íƒ€ì¼ ì „í™˜ ì§€ì›
- Negative Prompt ë¯¸ì§€ì› (CFG ë¯¸ì‚¬ìš©)
"""

import os
import sys

# [ì „ëµ 1] CUDA ë©”ëª¨ë¦¬ ì„¤ì •: torch import ì „ì— ì„¤ì •í•´ì•¼ íš¨ë ¥ì´ ìˆìŒ
os.environ["PYTORCH_ALLOC_CONF"] = "max_split_size_mb:256"

from typing import Dict, Any, Optional
from pathlib import Path
import torch
import gc
from PIL import Image
import threading # [ì „ëµ 2] ë™ì‹œì„± ì œì–´ìš©

from diffusers import (
    DiffusionPipeline,
    FlowMatchEulerDiscreteScheduler,
    AutoencoderKL
)

from .base import BaseNode
from ..config import (
    model_config,
    generation_config,
    aspect_ratio_templates,
)

# Z-Image Turbo ëª¨ë¸ ê²½ë¡œ
ZIT_MODELS_DIR = Path(os.getenv("ZIT_MODELS_DIR", "/opt/ai-models/zit"))
ZIT_BASE_MODEL = ZIT_MODELS_DIR / "Z-Image-Turbo-Base"
ZIT_LORA_DIR = ZIT_MODELS_DIR / "lora"

# ìŠ¤íƒ€ì¼ë³„ LoRA ë§¤í•‘
STYLE_LORA_MAP = {
    "realistic": None,  # ë² ì´ìŠ¤ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    "ultra_realistic": None,  # ë² ì´ìŠ¤ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    "semi_realistic": "OBåŠå†™å®è‚–åƒç”»2.0 OB Semi-Realistic Portraits z- image turbo(1).safetensors",
    "anime": "Anime-Z.safetensors",
}

# ==============================================================================
# â˜… ì „ì—­ ìƒíƒœ ê´€ë¦¬ (ìºì‹œ + ë½ + ì‹¤í–‰ ì¹´ìš´í„°)
# ==============================================================================
_GLOBAL_PIPE = None
_CURRENT_LORA = "init"
_EXECUTION_LOCK = threading.Lock() # [ì „ëµ 2] GPUëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ì‘ì—…ë§Œ ìˆ˜í–‰
_EXECUTION_COUNT = 0               # [ì „ëµ 4] ì£¼ê¸°ì  ì²­ì†Œë¥¼ ìœ„í•œ ì¹´ìš´í„°

class Text2ImageNode(BaseNode):
    def __init__(self, device: Optional[str] = None, auto_unload: bool = False):
        super().__init__("Text2ImageNode")
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.auto_unload = auto_unload

    def load_pipeline(self):
        global _GLOBAL_PIPE
        
        if _GLOBAL_PIPE is not None:
            return _GLOBAL_PIPE

        print(f"[{self.node_name}] ğŸš€ Initializing Pipeline (Enterprise Settings)...")
        try:
            scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
                str(ZIT_BASE_MODEL), subfolder="scheduler"
            )

            pipe = DiffusionPipeline.from_pretrained(
                str(ZIT_BASE_MODEL),
                scheduler=scheduler,
                torch_dtype=torch.bfloat16,  # ZITëŠ” bfloat16 í•„ìˆ˜ (float16ì€ NaN ìƒì„±)
                local_files_only=True,
                trust_remote_code=True,
                low_cpu_mem_usage=True  # ë¡œë”© ì‹œ RAM ìŠ¤íŒŒì´í¬ ë°©ì§€
            )

            # ComfyUI ìŠ¤íƒ€ì¼ ìµœì í™”
            # Model CPU Offload (Sequentialë³´ë‹¤ ë¹ ë¦„, ComfyUIì™€ ìœ ì‚¬)
            pipe.enable_model_cpu_offload(gpu_id=0)

            # Attention ìµœì í™” (ComfyUIì˜ pytorch attention)
            try:
                # PyTorch 2.0+ scaled dot product attention (FlashAttention)
                if hasattr(pipe, "unet") and hasattr(pipe.unet, "set_attn_processor"):
                    from diffusers.models.attention_processor import AttnProcessor2_0
                    pipe.unet.set_attn_processor(AttnProcessor2_0())
                elif hasattr(pipe, "transformer") and hasattr(pipe.transformer, "set_attn_processor"):
                    from diffusers.models.attention_processor import AttnProcessor2_0
                    pipe.transformer.set_attn_processor(AttnProcessor2_0())
            except Exception as e:
                print(f"[{self.node_name}] Could not enable attention optimization: {e}")

            # VAE Optimization (ê³ í•´ìƒë„ ëŒ€ì‘)
            pipe.vae.enable_tiling()
            pipe.vae.enable_slicing()

            print(f"[{self.node_name}] âœ… Model loaded with optimized CPU offload + attention")
            
            _GLOBAL_PIPE = pipe
            return pipe

        except Exception as e:
            raise RuntimeError(f"Pipeline Load Failed: {e}")

    def safe_unload_lora(self, pipe):
        """[ì „ëµ 4] ì•ˆì „í•˜ê²Œ LoRA ì–¸ë¡œë“œ (ë²„ì „ í˜¸í™˜ì„± ì²´í¬)"""
        if hasattr(pipe, "unload_lora_weights"):
            try:
                pipe.unload_lora_weights()
            except Exception as e:
                print(f"âš ï¸ Warning: unload_lora_weights failed: {e}")
        else:
            # unloadê°€ ì—†ëŠ” êµ¬ë²„ì „ ë“±ì—ì„œëŠ” fuse í•´ì œ ë“±ì„ ê³ ë ¤í•´ì•¼ í•˜ë‚˜ ZITëŠ” ìµœì‹ ì´ë¯€ë¡œ íŒ¨ìŠ¤
            print(f"âš ï¸ Warning: Pipeline has no unload_lora_weights method.")

    def switch_lora(self, pipe, style):
        global _CURRENT_LORA
        target_lora_file = STYLE_LORA_MAP.get(style)
        
        if _CURRENT_LORA == style: return

        # 1. Base Modelë¡œ ë³µê·€
        if target_lora_file is None:
            if _CURRENT_LORA is not None:
                print(f"[{self.node_name}] ğŸ”„ Switching to Base Model")
                self.safe_unload_lora(pipe)
                _CURRENT_LORA = style
            return

        # 2. ìƒˆë¡œìš´ LoRA ë¡œë“œ
        lora_path = ZIT_LORA_DIR / target_lora_file
        if lora_path.exists():
            print(f"[{self.node_name}] ğŸ”„ Loading LoRA: {style}")
            self.safe_unload_lora(pipe) # ê¸°ì¡´ ì œê±°
            try:
                pipe.load_lora_weights(str(lora_path))
                _CURRENT_LORA = style
            except Exception as e:
                print(f"âš ï¸ LoRA Load Error (Check filename/encoding): {e}")
                # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒíƒœë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ˆê¸°í™” í‘œì‹œ
                _CURRENT_LORA = "error" 
        else:
            print(f"âš ï¸ LoRA file missing: {lora_path}")

    def get_generator_device(self, pipe):
        """[ì „ëµ 5] Generatorë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ íƒìƒ‰"""
        # 1ìˆœìœ„: íŒŒì´í”„ë¼ì¸ì˜ ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (CPU Offload ì‹œ 'cuda'ê°€ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìŒ)
        if hasattr(pipe, "_execution_device"):
            return pipe._execution_device
        
        # 2ìˆœìœ„: ëª¨ë¸ì˜ device ì†ì„±
        if hasattr(pipe, "device"):
            # CPU Offload ìƒíƒœë©´ pipe.deviceê°€ 'cpu'ì¼ ìˆ˜ ìˆìŒ. 
            # í•˜ì§€ë§Œ ì‹¤ì œ ì—°ì‚°ì€ ê°€ì†ê¸°ì—ì„œ í•˜ë¯€ë¡œ CUDAê°€ ìˆë‹¤ë©´ CUDAë¥¼ ìš°ì„ í•´ì•¼ í•¨.
            if pipe.device.type == "cpu" and torch.cuda.is_available():
                return torch.device("cuda")
            return pipe.device

        # 3ìˆœìœ„: ì´ˆê¸°í™” ì‹œ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤
        return torch.device(self.device)

    def process(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        global _EXECUTION_COUNT

        # [ì „ëµ 2] ì“°ë ˆë“œ ë½: ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ì´ ì™€ë„ ìˆœì°¨ ì²˜ë¦¬ ë³´ì¥
        with _EXECUTION_LOCK:
            prompt = inputs["prompt"]
            aspect_ratio = inputs.get("aspect_ratio", generation_config.DEFAULT_ASPECT_RATIO)
            style = inputs.get("style", "realistic")
            num_inference_steps = inputs.get("num_inference_steps", 9)
            seed = inputs.get("seed", None)
            
            # [ì „ëµ 3] í•´ìƒë„ ë³´ì •: ë‚´ë¦¼(floor) ëŒ€ì‹  ë°˜ì˜¬ë¦¼(round) ì‚¬ìš©
            w, h = aspect_ratio_templates.get_size(aspect_ratio)
            width = int(round(w / 16) * 16)
            height = int(round(h / 16) * 16)
            
            if width != w or height != h:
                # ë¡œê·¸ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì°¨ì´ê°€ í´ ë•Œë§Œ ì¶œë ¥í•˜ê±°ë‚˜ ìƒëµ ê°€ëŠ¥
                pass

            # 1. íŒŒì´í”„ë¼ì¸ ì¤€ë¹„
            pipe = self.load_pipeline()
            #self.switch_lora(pipe, style)

            # 2. ì œë„ˆë ˆì´í„° ìƒì„±
            generator = None
            if seed is not None:
                exec_device = self.get_generator_device(pipe)
                generator = torch.Generator(device=exec_device).manual_seed(seed)

            print(f"[{self.node_name}] Generating ({width}x{height})...")
            
            # 3. ìƒì„±
            with torch.no_grad():
                image = pipe(
                    prompt=prompt,
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=0.0, 
                    generator=generator,
                    output_type="pil"
                ).images[0]

            # [ì „ëµ 4] ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ë²ˆ í•˜ë©´ ëŠë¦¼, 5íšŒë§ˆë‹¤ ì‹¤í–‰)
            _EXECUTION_COUNT += 1
            if _EXECUTION_COUNT % 5 == 0:
                print(f"[{self.node_name}] ğŸ§¹ Periodic Memory Cleanup (Count: {_EXECUTION_COUNT})")
                gc.collect()
                torch.cuda.empty_cache()

            # auto_unloadê°€ Trueì¼ ë•Œë§Œ ê°•ì œ ì¢…ë£Œ (ë³´í†µì€ Falseë¡œ ë‘ )
            if self.auto_unload:
                self.flush_global()

            return {"image": image, "seed": seed, "width": width, "height": height}
    
    def flush_global(self):
        """ì „ì—­ ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        global _GLOBAL_PIPE, _CURRENT_LORA
        if _GLOBAL_PIPE is not None:
            del _GLOBAL_PIPE
            _GLOBAL_PIPE = None
        _CURRENT_LORA = "init"
        gc.collect()
        torch.cuda.empty_cache()

    def get_required_inputs(self): return ["prompt"]
    def get_output_keys(self): return ["image", "seed", "width", "height"]