"""
Shared Model Component Cache for Z-Image Turbo
T2Iì™€ I2Iê°€ ë™ì¼í•œ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

ë©”ëª¨ë¦¬ íš¨ìœ¨:
- ë¶„ë¦¬ëœ ìºì‹œ: T2I (20.5GB) + I2I (20.5GB) = 41GB âŒ
- ê³µìœ  ìºì‹œ: Transformer + VAE + Text Encoder = 20.5GB âœ…
"""

import os
import gc
import threading
from pathlib import Path
from typing import Optional, Tuple

import torch
from diffusers import (
    ZImagePipeline,
    ZImageImg2ImgPipeline,
    FlowMatchEulerDiscreteScheduler,
    AutoencoderKL
)

from src.utils.logging import get_logger

logger = get_logger(__name__)

# ==============================================================================
# ì „ì—­ ê³µìœ  ì»´í¬ë„ŒíŠ¸ ìºì‹œ
# ==============================================================================
_GLOBAL_TRANSFORMER = None
_GLOBAL_VAE = None
_GLOBAL_TEXT_ENCODER = None
_GLOBAL_TOKENIZER = None
_GLOBAL_SCHEDULER = None
_CACHE_LOCK = threading.Lock()
_PIPELINE_LOCK = threading.Lock()  # íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì¤‘ ë½

# ëª¨ë¸ ê²½ë¡œ
ZIT_MODELS_DIR = Path(os.getenv("ZIT_MODELS_DIR", "/opt/ai-models/zit"))
ZIT_BASE_MODEL = ZIT_MODELS_DIR / "Z-Image-Turbo-BF16"


def load_shared_components(device: str = "cuda") -> Tuple:
    """
    ê³µìœ  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ (í•œ ë²ˆë§Œ)

    Returns:
        (transformer, vae, text_encoder, tokenizer, scheduler)
    """
    global _GLOBAL_TRANSFORMER, _GLOBAL_VAE, _GLOBAL_TEXT_ENCODER
    global _GLOBAL_TOKENIZER, _GLOBAL_SCHEDULER

    with _CACHE_LOCK:
        # ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë©´ ë°˜í™˜
        if _GLOBAL_TRANSFORMER is not None:
            logger.info("[SharedCache] âœ… Using cached components")
            return (
                _GLOBAL_TRANSFORMER,
                _GLOBAL_VAE,
                _GLOBAL_TEXT_ENCODER,
                _GLOBAL_TOKENIZER,
                _GLOBAL_SCHEDULER
            )

        logger.info(f"[SharedCache] ğŸš€ Loading ZIT components (20.5GB)...")

        # Scheduler ë¡œë“œ
        _GLOBAL_SCHEDULER = FlowMatchEulerDiscreteScheduler.from_pretrained(
            str(ZIT_BASE_MODEL), subfolder="scheduler"
        )

        # ì„ì‹œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ì»´í¬ë„ŒíŠ¸ ì¶”ì¶œìš©)
        temp_pipe = ZImagePipeline.from_pretrained(
            str(ZIT_BASE_MODEL),
            scheduler=_GLOBAL_SCHEDULER,
            torch_dtype=torch.bfloat16,
            local_files_only=True,
            low_cpu_mem_usage=True
        )
        
        # ì»´í¬ë„ŒíŠ¸ ì¶”ì¶œ
        _GLOBAL_TRANSFORMER = temp_pipe.transformer
        _GLOBAL_VAE = temp_pipe.vae
        _GLOBAL_TEXT_ENCODER = temp_pipe.text_encoder
        _GLOBAL_TOKENIZER = temp_pipe.tokenizer

        # GPUë¡œ ì´ë™
        _GLOBAL_TRANSFORMER.to(device)
        _GLOBAL_VAE.to(device)
        _GLOBAL_TEXT_ENCODER.to(device)

        # FlashAttention ìµœì í™”
        try:
            if hasattr(_GLOBAL_TRANSFORMER, "set_attn_processor"):
                from diffusers.models.attention_processor import AttnProcessor2_0
                _GLOBAL_TRANSFORMER.set_attn_processor(AttnProcessor2_0())
                logger.info(f"[SharedCache] FlashAttention enabled")
        except Exception as e:
            logger.info(f"[SharedCache] Could not enable attention optimization: {e}")

        # VAE ìµœì í™”
        _GLOBAL_VAE.enable_tiling()
        _GLOBAL_VAE.enable_slicing()
        logger.info(f"[SharedCache] VAE tiling/slicing enabled")

        # ì„ì‹œ íŒŒì´í”„ë¼ì¸ ì‚­ì œ
        del temp_pipe
        gc.collect()

        logger.info(f"[SharedCache] âœ… Components loaded on {device}")

        return (
            _GLOBAL_TRANSFORMER,
            _GLOBAL_VAE,
            _GLOBAL_TEXT_ENCODER,
            _GLOBAL_TOKENIZER,
            _GLOBAL_SCHEDULER
        )


def get_t2i_pipeline(device: str = "cuda") -> ZImagePipeline:
    """
    ê³µìœ  ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” T2I íŒŒì´í”„ë¼ì¸ ìƒì„±

    Returns:
        ZImagePipeline (ë©”ëª¨ë¦¬ ì¶”ê°€ ì‚¬ìš© ì—†ìŒ)
    """
    transformer, vae, text_encoder, tokenizer, scheduler = load_shared_components(device)

    pipe = ZImagePipeline(
        scheduler=scheduler,
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        transformer=transformer
    )

    # íŒŒì´í”„ë¼ì¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
    pipe.enable_attention_slicing()
    pipe.to(device)

    logger.info("[SharedCache] T2I pipeline created (using shared components)")
    return pipe


def get_i2i_pipeline(device: str = "cuda") -> ZImageImg2ImgPipeline:
    """
    ê³µìœ  ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” I2I íŒŒì´í”„ë¼ì¸ ìƒì„±

    Returns:
        ZImageImg2ImgPipeline (ë©”ëª¨ë¦¬ ì¶”ê°€ ì‚¬ìš© ì—†ìŒ)
    """
    transformer, vae, text_encoder, tokenizer, scheduler = load_shared_components(device)

    pipe = ZImageImg2ImgPipeline(
        scheduler=scheduler,
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        transformer=transformer
    )

    # íŒŒì´í”„ë¼ì¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
    pipe.enable_attention_slicing()
    pipe.to(device)
    
    logger.info("[SharedCache] I2I pipeline created (using shared components)")
    return pipe


def flush_shared_cache():
    """ì „ì—­ ìºì‹œ ì™„ì „ ì´ˆê¸°í™” (ì•ˆì „: íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì¤‘ ëŒ€ê¸°)"""
    global _GLOBAL_TRANSFORMER, _GLOBAL_VAE, _GLOBAL_TEXT_ENCODER
    global _GLOBAL_TOKENIZER, _GLOBAL_SCHEDULER

    # íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    with _PIPELINE_LOCK:
        with _CACHE_LOCK:
            if _GLOBAL_TRANSFORMER is not None:
                del _GLOBAL_TRANSFORMER
                del _GLOBAL_VAE
                del _GLOBAL_TEXT_ENCODER
                del _GLOBAL_TOKENIZER
                del _GLOBAL_SCHEDULER

                _GLOBAL_TRANSFORMER = None
                _GLOBAL_VAE = None
                _GLOBAL_TEXT_ENCODER = None
                _GLOBAL_TOKENIZER = None
                _GLOBAL_SCHEDULER = None

                gc.collect()
                torch.cuda.empty_cache()
                logger.info("[SharedCache] âœ… Cache flushed")


def is_cache_loaded() -> bool:
    """ìºì‹œ ë¡œë“œ ìƒíƒœ í™•ì¸"""
    return _GLOBAL_TRANSFORMER is not None


def get_pipeline_lock():
    """íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì‹œ ë½ì„ íšë“í•˜ëŠ” context manager"""
    return _PIPELINE_LOCK
