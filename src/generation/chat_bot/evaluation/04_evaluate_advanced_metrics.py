"""
í‰ê°€ ìŠ¤í¬ë¦½íŠ¸: ê³ ê¸‰ í‰ê°€ ì§€í‘œ (Relevance@K, NDCG@K, Success Rate, Answer Quality)

ëª©ì :
- v5.9 ì¿¼ë¦¬ë¡œ Recall ì´ì™¸ì˜ í‰ê°€ ì§€í‘œ ì¸¡ì •
- Relevance@K: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜
- NDCG@K: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰ í’ˆì§ˆ
- Success Rate: ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹µë³€ ìƒì„± ê°€ëŠ¥ ë¹„ìœ¨
- Answer Quality: LLM-as-Judgeë¡œ ë‹µë³€ í’ˆì§ˆ í‰ê°€ (1-5ì )

ì‹¤í–‰:
  OPENAI_API_KEY=xxx python evaluation/10_advanced_metrics_eval.py

ì¶œë ¥:
  evaluation/results/v59_advanced_metrics_results.json
"""

import json
import os
import sys
import time
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Any, Optional

import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
os.environ.setdefault("OMP_NUM_THREADS", "2")
os.environ.setdefault("MKL_NUM_THREADS", "2")

# --------------------------------------------
# ì„¤ì •
# --------------------------------------------
QUERIES_PATH = PROJECT_ROOT / "evaluation" / "results" / "synthetic_queries_v59.json"
OUTPUT_PATH = PROJECT_ROOT / "evaluation" / "results" / "v59_advanced_metrics_results.json"
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

# OpenAI API Key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("Success Rateì™€ Answer Quality í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# í‰ê°€í•  K ê°’ë“¤
K_VALUES = [1, 3, 5, 10]

# LLM ì„¤ì •
LLM_MODEL = "gpt-4o-mini"


# --------------------------------------------
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# --------------------------------------------
def init_rag():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    from rag.chain import SmallBizRAG

    rag = SmallBizRAG(use_reranker=False)
    print(f"âœ… RAG ë¡œë“œ ì™„ë£Œ: {rag.vectorstore._collection.count()}ê°œ ë¬¸ì„œ\n")

    return rag


# --------------------------------------------
# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# --------------------------------------------
def init_llm():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if not OPENAI_API_KEY:
        return None

    print("ğŸ”§ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
    from openai import OpenAI

    client = OpenAI(api_key=OPENAI_API_KEY)
    print(f"âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {LLM_MODEL})\n")

    return client


# --------------------------------------------
# ê²€ìƒ‰ í•¨ìˆ˜
# --------------------------------------------
def search_and_get_docs(rag, query: str, k: int = 10) -> List[Any]:
    """ê²€ìƒ‰ ì‹¤í–‰ ë° ë¬¸ì„œ ê°ì²´ ë°˜í™˜"""
    docs = rag.vectorstore.similarity_search(query, k=k)
    return docs


# --------------------------------------------
# Relevance@K ê³„ì‚°
# --------------------------------------------
def calculate_relevance_at_k(
    retrieved_docs: List[Any],
    ground_truth: Dict[str, int],
    k: int
) -> float:
    """Relevance@K ê³„ì‚°

    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (LangChain Document)
        ground_truth: {doc_id: relevance} ë§¤í•‘ (relevance: 0-2)
        k: Top K

    Returns:
        í‰ê·  relevance ì ìˆ˜ (0~2)
    """
    if not retrieved_docs:
        return 0.0

    top_k_docs = retrieved_docs[:k]
    relevances = []

    for doc in top_k_docs:
        doc_id = doc.metadata.get("doc_id", "")
        relevance = ground_truth.get(doc_id, 0)
        relevances.append(relevance)

    return np.mean(relevances) if relevances else 0.0


# --------------------------------------------
# NDCG@K ê³„ì‚°
# --------------------------------------------
def calculate_dcg(relevances: List[float]) -> float:
    """DCG (Discounted Cumulative Gain) ê³„ì‚°"""
    dcg = 0.0
    for i, rel in enumerate(relevances, start=1):
        dcg += rel / np.log2(i + 1)
    return dcg


def calculate_ndcg_at_k(
    retrieved_docs: List[Any],
    ground_truth: Dict[str, int],
    k: int
) -> float:
    """NDCG@K ê³„ì‚°

    NDCG = DCG / IDCG
    - DCG: ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ì˜ ì ìˆ˜
    - IDCG: ì´ìƒì ì¸ ìˆœì„œì˜ ì ìˆ˜ (relevance ë†’ì€ ìˆœ)
    """
    if not retrieved_docs:
        return 0.0

    top_k_docs = retrieved_docs[:k]

    # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ì˜ relevance ìˆœì„œ
    actual_relevances = []
    for doc in top_k_docs:
        doc_id = doc.metadata.get("doc_id", "")
        relevance = ground_truth.get(doc_id, 0)
        actual_relevances.append(relevance)

    # ì´ìƒì ì¸ relevance ìˆœì„œ (ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
    ideal_relevances = sorted(ground_truth.values(), reverse=True)[:k]

    # DCG ê³„ì‚°
    dcg = calculate_dcg(actual_relevances)
    idcg = calculate_dcg(ideal_relevances)

    # NDCG ê³„ì‚°
    if idcg == 0.0:
        return 0.0

    return dcg / idcg


# --------------------------------------------
# Success Rate í‰ê°€
# --------------------------------------------
def evaluate_success_rate(
    llm_client,
    query: str,
    retrieved_docs: List[Any]
) -> bool:
    """Success Rate í‰ê°€: ê²€ìƒ‰ ê²°ê³¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ ê°€ëŠ¥í•œê°€?"""
    if not llm_client or not retrieved_docs:
        return False

    # ë¬¸ì„œ ë‚´ìš© ìš”ì•½
    doc_summaries = []
    for i, doc in enumerate(retrieved_docs[:5], 1):  # Top 5ë§Œ ì‚¬ìš©
        title = doc.metadata.get("title", "")
        location = doc.metadata.get("location", "")
        industry = doc.metadata.get("industry", "")
        text_snippet = doc.page_content[:200]  # ì²˜ìŒ 200ì

        summary = f"{i}. {title} ({location}, {industry})\n{text_snippet}..."
        doc_summaries.append(summary)

    docs_text = "\n\n".join(doc_summaries)

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""ë‹¤ìŒì€ ì†Œìƒê³µì¸ ë§ˆì¼€íŒ… ìƒë‹´ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë§¤ì¥ ì •ë³´ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë§¤ì¥ ì •ë³´:
{docs_text}

ì´ ë§¤ì¥ ì •ë³´ë“¤ì„ í™œìš©í•˜ì—¬ ìœ„ ì§ˆë¬¸ì— ìœ ì˜ë¯¸í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆë‚˜ìš”?

ë‹µë³€ ê¸°ì¤€:
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë§¤ì¥ ì •ë³´ê°€ 1ê°œ ì´ìƒ ìˆìœ¼ë©´ "ì˜ˆ"
- ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ì œê³µ ê°€ëŠ¥í•˜ë©´ "ì˜ˆ"
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ê³¼ ì „í˜€ ë¬´ê´€í•˜ë©´ "ì•„ë‹ˆì˜¤"

ë‹µë³€: "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    try:
        response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=10,
        )

        answer = response.choices[0].message.content.strip()
        return "ì˜ˆ" in answer or "yes" in answer.lower()

    except Exception as e:
        print(f"  âš ï¸  Success Rate í‰ê°€ ì‹¤íŒ¨: {e}")
        return False


# --------------------------------------------
# Answer Quality í‰ê°€
# --------------------------------------------
def evaluate_answer_quality(
    llm_client,
    query: str,
    retrieved_docs: List[Any]
) -> Optional[int]:
    """Answer Quality í‰ê°€: LLM-as-Judgeë¡œ ë‹µë³€ í’ˆì§ˆ í‰ê°€ (1-5ì )"""
    if not llm_client or not retrieved_docs:
        return None

    # 1. ë‹µë³€ ìƒì„±
    doc_summaries = []
    for i, doc in enumerate(retrieved_docs[:5], 1):
        title = doc.metadata.get("title", "")
        location = doc.metadata.get("location", "")
        industry = doc.metadata.get("industry", "")
        review_count = doc.metadata.get("review_count", 0)
        rating = doc.metadata.get("rating", 0.0)
        text_snippet = doc.page_content[:300]

        summary = f"{i}. {title} ({location}, {industry})\n   ë¦¬ë·°: {review_count}ê°œ, í‰ì : {rating}\n   {text_snippet}..."
        doc_summaries.append(summary)

    docs_text = "\n\n".join(doc_summaries)

    generation_prompt = f"""ë‹¤ìŒì€ ì†Œìƒê³µì¸ ë§ˆì¼€íŒ… ìƒë‹´ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë§¤ì¥ ì •ë³´ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë§¤ì¥ ì •ë³´:
{docs_text}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë§ˆì¼€íŒ… ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
- êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ í•¨ê»˜ ì„¤ëª…
- ì‹¤í–‰ ê°€ëŠ¥í•œ íŒ ì œê³µ
- 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ"""

    try:
        # ë‹µë³€ ìƒì„±
        generation_response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": generation_prompt}],
            temperature=0.7,
            max_tokens=300,
        )

        generated_answer = generation_response.choices[0].message.content.strip()

        # 2. ë‹µë³€ í’ˆì§ˆ í‰ê°€ (LLM-as-Judge)
        judge_prompt = f"""ë‹¤ìŒì€ ì†Œìƒê³µì¸ ë§ˆì¼€íŒ… ìƒë‹´ ì§ˆë¬¸ê³¼ AIê°€ ìƒì„±í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

AI ë‹µë³€:
{generated_answer}

ì´ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
- 5ì : ì§ˆë¬¸ì— ì™„ë²½íˆ ë‹µí•˜ë©°, êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ ì œê³µ
- 4ì : ì§ˆë¬¸ì— ì˜ ë‹µí•˜ë©°, ìœ ìš©í•œ ì •ë³´ í¬í•¨
- 3ì : ì§ˆë¬¸ì— ë‹µí•˜ì§€ë§Œ ë‹¤ì†Œ ì¼ë°˜ì ì´ê±°ë‚˜ ë¶€ì¡±í•¨
- 2ì : ì§ˆë¬¸ê³¼ ê´€ë ¨ì€ ìˆìœ¼ë‚˜ ë„ì›€ì´ ì œí•œì 
- 1ì : ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ê±°ë‚˜ ì˜ëª»ëœ ì •ë³´

ì ìˆ˜ë§Œ ì¶œë ¥í•˜ì„¸ìš” (1, 2, 3, 4, ë˜ëŠ” 5):"""

        judge_response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": judge_prompt}],
            temperature=0.0,
            max_tokens=5,
        )

        score_text = judge_response.choices[0].message.content.strip()

        # ì ìˆ˜ ì¶”ì¶œ
        for char in score_text:
            if char.isdigit():
                score = int(char)
                if 1 <= score <= 5:
                    return score

        return None

    except Exception as e:
        print(f"  âš ï¸  Answer Quality í‰ê°€ ì‹¤íŒ¨: {e}")
        return None


# --------------------------------------------
# í‰ê°€ ì‹¤í–‰
# --------------------------------------------
def evaluate_all_metrics(rag, llm_client, queries: List[Dict[str, Any]]):
    """ëª¨ë“  ë©”íŠ¸ë¦­ í‰ê°€"""
    print("=" * 80)
    print("ğŸ” í‰ê°€ ì‹œì‘")
    print("=" * 80)

    results = {
        f"relevance@{k}": [] for k in K_VALUES
    }
    results.update({
        f"ndcg@{k}": [] for k in K_VALUES
    })
    results["success_rate"] = []
    results["answer_quality"] = []

    # ì˜ë„ë³„ ê²°ê³¼
    intent_results = defaultdict(lambda: {
        **{f"relevance@{k}": [] for k in K_VALUES},
        **{f"ndcg@{k}": [] for k in K_VALUES},
        "success_rate": [],
        "answer_quality": [],
    })

    for idx, query_item in enumerate(queries):
        query = query_item["query"]
        intent = query_item.get("intent", "unknown")
        relevant_docs = query_item.get("relevant_docs", [])

        # Ground truth: {doc_id: relevance}
        ground_truth = {
            doc["doc_id"]: doc.get("relevance", 0)
            for doc in relevant_docs
        }

        if not ground_truth:
            continue

        # ê²€ìƒ‰ ì‹¤í–‰
        retrieved_docs = search_and_get_docs(rag, query, k=max(K_VALUES))

        # Relevance@K ê³„ì‚°
        for k in K_VALUES:
            rel_k = calculate_relevance_at_k(retrieved_docs, ground_truth, k)
            results[f"relevance@{k}"].append(rel_k)
            intent_results[intent][f"relevance@{k}"].append(rel_k)

        # NDCG@K ê³„ì‚°
        for k in K_VALUES:
            ndcg_k = calculate_ndcg_at_k(retrieved_docs, ground_truth, k)
            results[f"ndcg@{k}"].append(ndcg_k)
            intent_results[intent][f"ndcg@{k}"].append(ndcg_k)

        # Success Rate (LLM í˜¸ì¶œ)
        if llm_client:
            success = evaluate_success_rate(llm_client, query, retrieved_docs)
            results["success_rate"].append(1.0 if success else 0.0)
            intent_results[intent]["success_rate"].append(1.0 if success else 0.0)

            # Answer Quality (LLM í˜¸ì¶œ)
            quality = evaluate_answer_quality(llm_client, query, retrieved_docs)
            if quality is not None:
                results["answer_quality"].append(quality)
                intent_results[intent]["answer_quality"].append(quality)

            # Rate limiting
            time.sleep(0.1)

        # ì§„í–‰ ìƒí™©
        if (idx + 1) % 20 == 0:
            print(f"ì§„í–‰: {idx + 1}/{len(queries)}")

    print(f"âœ… í‰ê°€ ì™„ë£Œ: {len(queries)}ê°œ ì¿¼ë¦¬\n")

    # í‰ê·  ê³„ì‚°
    avg_results = {}
    for metric, values in results.items():
        if values:
            avg_results[metric] = sum(values) / len(values)
        else:
            avg_results[metric] = 0.0

    # ì˜ë„ë³„ í‰ê· 
    intent_avg = {}
    for intent, metrics_dict in intent_results.items():
        intent_avg[intent] = {}
        for metric, values in metrics_dict.items():
            if values:
                intent_avg[intent][metric] = sum(values) / len(values)
            else:
                intent_avg[intent][metric] = 0.0

    return {
        "overall": avg_results,
        "by_intent": intent_avg,
        "raw": results,
    }


# --------------------------------------------
# ê²°ê³¼ ì¶œë ¥
# --------------------------------------------
def print_results(results: Dict[str, Any], total_queries: int):
    """ê²°ê³¼ ì¶œë ¥"""
    print("=" * 80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼")
    print("=" * 80)

    overall = results["overall"]

    # 1. Relevance@K
    print("\n### Relevance@K (í‰ê·  ê´€ë ¨ì„± ì ìˆ˜, 0-2)")
    print(f"{'Metric':<15} {'Score':>10}")
    print("-" * 26)
    for k in K_VALUES:
        score = overall.get(f"relevance@{k}", 0.0)
        print(f"Relevance@{k:<2}    {score:>10.3f}")

    # 2. NDCG@K
    print("\n### NDCG@K (ìˆœìœ„ í’ˆì§ˆ, 0-1)")
    print(f"{'Metric':<15} {'Score':>10}")
    print("-" * 26)
    for k in K_VALUES:
        score = overall.get(f"ndcg@{k}", 0.0)
        print(f"NDCG@{k:<2}         {score:>10.3f}")

    # 3. Success Rate & Answer Quality
    if overall.get("success_rate", 0.0) > 0:
        print("\n### Success Rate & Answer Quality")
        print(f"{'Metric':<20} {'Score':>10}")
        print("-" * 31)

        success_rate = overall.get("success_rate", 0.0)
        print(f"Success Rate        {success_rate:>9.1%}")

        if overall.get("answer_quality", 0.0) > 0:
            quality = overall.get("answer_quality", 0.0)
            print(f"Answer Quality      {quality:>10.2f}/5")

    # 4. ì˜ë„ë³„ ê²°ê³¼
    print("\n### ì˜ë„ë³„ ì„±ëŠ¥")
    print(f"{'Intent':<20} {'Rel@5':>8} {'NDCG@5':>8} {'Success':>8} {'Quality':>8}")
    print("-" * 62)

    by_intent = results["by_intent"]
    for intent, metrics in by_intent.items():
        rel5 = metrics.get("relevance@5", 0.0)
        ndcg5 = metrics.get("ndcg@5", 0.0)
        success = metrics.get("success_rate", 0.0)
        quality = metrics.get("answer_quality", 0.0)

        quality_str = f"{quality:.2f}" if quality > 0 else "-"
        print(f"{intent:<20} {rel5:>7.3f} {ndcg5:>7.3f} {success:>7.1%} {quality_str:>8}")

    print(f"\nì´ ì¿¼ë¦¬ ìˆ˜: {total_queries}")
    print("=" * 80)


# --------------------------------------------
# ë©”ì¸
# --------------------------------------------
def main():
    # ì¿¼ë¦¬ ë¡œë“œ
    print("ğŸ“‹ ì¿¼ë¦¬ ë¡œë“œ ì¤‘...")
    with open(QUERIES_PATH, "r", encoding="utf-8") as f:
        queries_data = json.load(f)

    queries = queries_data.get("queries", [])
    print(f"âœ… {len(queries)}ê°œ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ\n")

    # RAG ì´ˆê¸°í™”
    rag = init_rag()

    # LLM ì´ˆê¸°í™”
    llm_client = init_llm()

    # í‰ê°€ ì‹¤í–‰
    results = evaluate_all_metrics(rag, llm_client, queries)

    # ê²°ê³¼ ì¶œë ¥
    print_results(results, len(queries))

    # ê²°ê³¼ ì €ì¥
    output = {
        "version": "v5.9",
        "total_queries": len(queries),
        "k_values": K_VALUES,
        "overall": results["overall"],
        "by_intent": results["by_intent"],
    }

    with OUTPUT_PATH.open("w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {OUTPUT_PATH}")
    print("=" * 80)


if __name__ == "__main__":
    main()
